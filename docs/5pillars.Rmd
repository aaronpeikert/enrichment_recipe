---
title: "The five pillars of computational reproducibility"
author: "Mark Ziemann"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    fig_width: 5
    fig_height: 5
bibliography: 5pillars.bib
csl: plos-computational-biology.csl
---

Mark Ziemann

Deakin University, Geelong, Australia, School of Life and Environmental Sciences.

Correspondance: m.ziemann@deakin.edu.au

## Summary

Computational reproducibility is in theory a simple premise, however in practice it is difficult to
achieve.
Here, we present a framework to improve reproducibility, called the five pillars.
These are a combination of best practices which ensure that the computational research work can be
reproduced quickly and easily, and will endure relatively long time-frames.

## Introduction

```{r,libs,echo=FALSE}

library("DiagrammeR")

```

In research, computational reproducibility is the ability to use the materials from a past study
(such as data, code and documentation) to regenerate the outputs including figures and tables to
confirm the study's findings.
Reproducibility is only the first step towards overall reliability, for example a study may be
reproducible, but suffer from analytical problems that invalidate the results.
If a study is reproducible, then at least these analytical issues can be more easily identified and
rectified.
So reproducibility is a necessary stepping-stone that in theory should enhance the overall reliability
of computational research including replicability and robustness.
Replicability being defined as the ability to repeat the entire study and come up with similar overall
results.
Robustness refers to the ability for the findings to be generalisable to other situations
@Nosek2022-zc.

Unreliable research is a waste of resources and time, and can mislead readers to pursue doomed
research directions.
Much has been written on how irreproducibility has reached crisis levels in psychology and life
sciences [@Baker2015-bs; @Begley2012-di].
Indeed in bioinformatics, the situation is grim with only a very fraction of studies
being able to be reproduced @Ioannidis2009-hv, which brings into question the reliability of those
studies.
Irreproducible bioinformatics has the potential to place patient safety at risk, as demonstrated by an
earlier case study @Baggerly2010-nr.

As computational researchers, we are theoretically able to achieve 100% analytical reproducibility,
however in practice this is rarely achieved.
Data and code availability, ability to install software easily, and other factors mean that most
computational research works are unable to be reproduced.

A number of guides recommending enhanced computational reproducibility practices have emerged
[@Peng2011-ia; @Tan2010-is; @Sandve2013-lk; @Piccolo2016-ji; @Gruning2018-sg; @Lewis2016-qh;
@Brito2020-ld; @Akalin2018-eg ]
and this article attempts to unify these concepts and assemble a group of best practices where
computational reproducibility can be considered as complete, using a framework called the
five pillars of reproducible computational research (**Figure 1**).
These concepts are unpacked in the following sections.

```{r,5pillars,fig.cap="Figure 1. The five pillars of computational reproducibility.",echo=FALSE,fig.height=6}

library("RColorBrewer")
cols <- brewer.pal(5, "Set2")
cols2 <- adjustcolor( cols, alpha.f = 0.7)
plot(0,type='n',axes=FALSE,ann=FALSE, xlim=c(0, 10), ylim=c(0, 8.5))
polygon(c(0,1.8,1.8,0),c(1,1,6,6), col=cols2[1], border = NA)
polygon(c(2,3.8,3.8,2),c(1,1,6,6), col=cols2[2], border = NA)
polygon(c(4,5.8,5.8,4),c(1,1,6,6), col=cols2[3], border = NA)
polygon(c(6,7.8,7.8,6),c(1,1,6,6), col=cols2[4], border = NA)
polygon(c(8,9.8,9.8,8),c(1,1,6,6), col=cols2[5], border = NA)
polygon(c(-0.2,10,10,-0.2),c(-0.4,-0.4,0.8,0.8),col="lightgray", border = NA)
polygon(c(-0.2,5,10),c(6.5,8.5,6.5),col="lightgray", border = NA)
points( c(0.9,2.9,4.9,6.9,8.9) , c(4.5,2.5,4.5,2.5,4.5), pch=19,cex=18,col=cols )
text(5,7,lab="Five pillars of reproducible computational research", cex=1.5)

labs <- c("Code version\ncontrol &\npersistent\n sharing",
  "Compute \nenvironment \ncontrol",
  "Literate\nprogramming",
  "Documentation",
  "FAIR &\npersistent\ndata sharing")

text( c(0.9,2.9,4.9,6.9,8.9) , c(4.5,2.5,4.5,2.5,4.5) , lab=labs)
text( 9, -0.2 , lab=expression(italic("created by Mark Ziemann")),cex=0.5)

```

## 1. Code version control and persistent sharing

A version control system is a type of program that "tracks changes" made to sets of files, typically
other computer programs, scripts and documentation.
A set of files under version control is called a "repository" and represents a project or sub-project.
Version control is used extensively by software developers and is considered one of the key best
practices in software engineering.
Computational researchers can benefit significantly by adopting version control into their routine
practices.
Below I outline some of the benefits.

* All steps of the workflow are coded in a computer script that allows automation.
There are no manual intervention steps.
For example, spreadsheets are irreproducible and prone to data entry, manipulation and formula errors.
Even gene names can be automatically converted to dates, and this is common in publlished supplementary
files.
Includes data processing, and all figures and tables that go into the the final manuscript.
If there are manual intervention steps, such as data cleaning or curating metadata, then the cleaned
data can be shared and incorporated into the workflow.

* When designed correctly, the code "knows" where to access the associated metadata and data sets are,
which avoids "file not found" errors.
Metadata are usually small, so they can also be included in the repository as flat text files.
Small data files can also be included in the repository, although keep in mind that big data files
(>10MB) such as raw array scans, fastq files and the like are unsuitable for retention in a version
control repository as the large files will slow down code updates signifiantly.
Larger data files belong in a dedicated data repository, discussed below.
If the metadata and small data require some cleaning, then include the original and cleaned data in the
repository.
The code should refer only to the cleaned data and the original files are there only for reference.

* For a research project, code gets really complicated very quickly.
Workflows are build in a step wise way and require step-by-step validation that they are actually doing
what we want them to do.
There may be separate scripts for data processing, modeling and visualsation.
There may be many false starts, research ideas that were pursued, but didn't give desired results and
were removed from the final manuscript.
A project that is under development will have code that undergoes many changes each week over the course
of months to years.
There may be many contributors to a study and merging these code changes can get difficult without
version control.

* Version control keeps a track of all these changes over time.
We always know which version of the script is the most up-to-date.
We can inspect the code base at a particular point in the past.
Commit messages are retained, which gives a record of the motivations and purpose behind each
modification.

* Version control also works extremely well for documentation.
Books, educational resources and other data types are well served using version control.

* Using a centralised code hosting platform such as "GitHub" or "BitBucket" can help manage contributions
from team members who may be working ascynchrously in different timezones and on different servers.
It also makes the user's work searchable, which is convenient when code snippets from a project
need to be reused for a new project.
These centralised platforms also enable discussions of code "issues" such as bugs and feature requests.
In bioinformatics teams, issues can be used to track the progress of a project through it's milestones
and allows team members to raise potential issues with a colleagues code (or one's own).

* Version control makes works easily sharable.
Work is easier to reproduce because the source folder contains a complete workflow with linked code
that doesn't need any modifications before being executed to reproduce some works.

* `git` is the defacto standard for version control currently although there are alternatives like
`svn` (Subversion) and `hg` (Mercurial).
Using `git` is advisable because there is a greater depth of support available on web forums, and more
alternatives for centralised code hosting providers and graphical interfaces.

## 2. Compute environment control

* Conda

* Containers: Docker/Guix/others

* Virtual machines

* Wasm

## 3. Literate programming

The report is truth.

* Sweave, Rmarkdown, Quarto

* Jupyter

* Compilation

* Posting reports to lab website and webarchive

* Make the manuscript reproducible as well.

## 4. Documentation

* Instructions needed

* Requirements

* Credits

## 5. FAIR and persistent data sharing

* NCBI

* Zenodo

* Figshare

* Arweave/IPFS

* Portico

## Bringing it all together

Extreme reproducibility should be the new standard.

It generally doesn't matter what type of computer you're using.

The benefit is that even if one of the pillars breaks, the overall study could be reproduced
thanks to the remaining pillars.

More systematic studies to assess reproducible practices are required.



## Acknowledgements

I am grateful to the individuals who provided feedback on an [earlier draft of the five pillars
concept](https://twitter.com/mdziemann/status/1650371743113175040) including:

* Sriganesh Srihari

* Barbara Zdrazil

* Aziz Khan

* Pierre Poulain

* Irene LÃ³pez Santiago

## Bibliography

