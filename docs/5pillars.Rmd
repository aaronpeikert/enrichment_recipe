---
title: "The five pillars of computational reproducibility"
author: "Mark Ziemann"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    fig_width: 5
    fig_height: 5
bibliography: 5pillars.bib
csl: plos-computational-biology.csl
---

Mark Ziemann

Deakin University, Geelong, Australia, School of Life and Environmental Sciences.

Correspondance: m.ziemann@deakin.edu.au

## Summary

Computational reproducibility is in theory a simple premise, however in practice it is difficult to
achieve.
Here, we present a framework to improve reproducibility, called the five pillars.
These are a combination of best practices which ensure that the computational research work can be
reproduced quickly and easily, and will endure relatively long time-frames.

## Introduction

```{r,libs,echo=FALSE}

library("DiagrammeR")

```

In research, computational reproducibility is the ability to use the materials from a past study
(such as data, code and documentation) to regenerate the outputs including figures and tables to
confirm the study's findings.
Reproducibility is only the first step towards overall reliability, for example a study may be
reproducible, but suffer from analytical problems that invalidate the results.
If a study is reproducible, then at least these analytical issues can be more easily identified and
rectified.
So reproducibility is a necessary stepping-stone that in theory should enhance the overall reliability
of computational research including replicability and robustness.
Replicability being defined as the ability to repeat the entire study and come up with similar overall
results.
Robustness refers to the ability for the findings to be generalisable to other situations
@Nosek2022-zc.

Unreliable research is a waste of resources and time, and can mislead readers to pursue doomed
research directions.
Much has been written on how irreproducibility has reached crisis levels in psychology and life
sciences [@Baker2015-bs; @Begley2012-di].
Indeed in bioinformatics, the situation is grim with a 2009 report showing only a small fraction
of studies able to be reproduced @Ioannidis2009-hv, which brings into question the reliability of
those studies.
Irreproducible bioinformatics has the potential to place patient safety at risk, as demonstrated by an
earlier case study @Baggerly2010-nr.

As computational researchers, we are theoretically able to achieve 100% analytical reproducibility,
however in practice this is rarely achieved.
Data and code availability, ability to install software easily, and other factors mean that most
computational research works are unable to be reproduced.

A number of guides recommending enhanced computational reproducibility practices have emerged
[@Peng2011-ia; @Tan2010-is; @Sandve2013-lk; @Piccolo2016-ji; @Gruning2018-sg; @Lewis2016-qh;
@Brito2020-ld; @Akalin2018-eg ]
and this article attempts to unify these concepts and assemble a group of best practices where
computational reproducibility can be considered as complete, using a framework called the
five pillars of reproducible computational research (**Figure 1**).
These concepts are unpacked in the following sections.

```{r,5pillars,fig.cap="Figure 1. The five pillars of computational reproducibility.",echo=FALSE,fig.height=6}

library("RColorBrewer")
cols <- brewer.pal(5, "Set2")
cols2 <- adjustcolor( cols, alpha.f = 0.7)
plot(0,type='n',axes=FALSE,ann=FALSE, xlim=c(0, 10), ylim=c(0, 8.5))
polygon(c(0,1.8,1.8,0),c(1,1,6,6), col=cols2[1], border = NA)
polygon(c(2,3.8,3.8,2),c(1,1,6,6), col=cols2[2], border = NA)
polygon(c(4,5.8,5.8,4),c(1,1,6,6), col=cols2[3], border = NA)
polygon(c(6,7.8,7.8,6),c(1,1,6,6), col=cols2[4], border = NA)
polygon(c(8,9.8,9.8,8),c(1,1,6,6), col=cols2[5], border = NA)
polygon(c(-0.2,10,10,-0.2),c(-0.4,-0.4,0.8,0.8),col="lightgray", border = NA)
polygon(c(-0.2,5,10),c(6.3,8.5,6.3),col="lightgray", border = NA)
points( c(0.9,2.9,4.9,6.9,8.9) , c(4.5,2.5,4.5,2.5,4.5), pch=19,cex=18,col=cols )
text(5,7,lab="Five pillars of reproducible computational research", cex=1.5)

labs <- c("Code version\ncontrol &\npersistent\n sharing",
  "Compute \nenvironment \ncontrol",
  "Literate\nprogramming",
  "Documentation",
  "FAIR &\npersistent\ndata sharing")

text( c(0.9,2.9,4.9,6.9,8.9) , c(4.5,2.5,4.5,2.5,4.5) , lab=labs)
text( 5 , 0.3 , lab="End-to-end code coverage")
text( 9, -0.2 , lab=expression(italic("created by Mark Ziemann")),cex=0.5)

```

## End-to-end code coverage

End-to-end code coverage is a foundation for reproducibility as the codebase should contain all
instructions for completing a data analysis task.
This removes the need for any manual steps, which are not only time-consuming but also prone to errors.
Scripts enable better auditing and easier reproduction which would be difficult for graphical tools
like spreadsheets.
Indeed spreadsheets are prone to data entry, manipulation and formula errors @Panko1996-ko, and are
somewhat overused in the life sciences.
Transfering data between compute platforms is also discouraged.
For example, while there are extensive bioinformatic tools on Galaxy, GenePattern and other web
sources, the data needs to be transfered between platforms which is inefficient and could lead to
errors.
One thing to keep in mind is that, data cleaning and curation is often required, and in many cases
can only be conducted using manual intervention.
In those cases, retaining raw and cleaned data and a cleaning protocol is recommended.

Another principle of the five pillars approach is that the publishing researchers should put in the
extra effort over and above the minimum standards of a classical journal to make the reproduction
process easier and faster for those who wish to subsequently reproduce the analysis.

## Literate programming

Literate programming combines "chunks" of analytical code with human readable text @Knuth1984-di.
After compilation, the resulting output document contains the code together with computational results
such as figures and tables along with contextualising explanations and narratives.

The Sweave project, concieved in 2001, was designed to give LaTeX documents embedded R code chunks and
was envisaged as a way to generate R documentation materials and generate statistical analysis reports
@Leisch2002-yc.
Around 2015, R Markdown emerged as a more popular alternative, as formatting is simpler as compared to
Sweave, saving time.
Around the same time, Project Jupyter was developed to provide a "notebook" type interface, incorporating
R, Python, Julia and other computer language chunks into documents @Perkel2018-rz.
In 2022, the company behind RStudio (Posit) released Quarto, the conceptual successor of R Markdown,
but with enhanced support for other computer languages like Python, Julia, and Observable JavaScript
@Bauer2023-eh.

Whether Rmarkdown, Jupyter or Quarto, literate programming offers some substantial benefits over
alternative approaches:

* The provenance of any result can be demonstrated.
The resulting document produced contains the code executed, together with the results (eg: a chart).
This is in contrast to a data analysis report assembled in a word processor, which due to the many
copy-paste operations required might contain errors, or version mismatches.

* It saves time.
For the case where a report needs to be run routinely, this would involve significant copy-paste to
be assembled "manually" with a word processor.
Using a literate script would mean only minimal changes are required between interations.

* Literate programs accommodate extensive documentation.
This allows the analyst to include rich descriptions of scientific works.
For example, a report may contain background, methods, results, discussion and references.
Not only text, but various objects can be included like links to other resources,
tables, images, videos, etc.
More information on documentation below.

* Outputs are arranged.
When using a regular script, an analyst might make dozens of charts which are written to the
disk with names like "chart1.png" or "model8.svg".
When the number of outputs is large, it becomes unwieldy and hard to place which plot corresponds
to which part of the script.
By embedding the outputs (eg: charts and tables) in a document in the sequence that they were generated,
it helps the reader understand the logical steps taken in any analysis.

* Literate scripts are amenable to version control.
Version control is a useful best practice in software development (discussed below).

* Literate programming output reports are free from code errors.
The resulting document is only rendered when the entire script has compiled without errors.
In contrast, a regular script might generate some outputs and then encounter an error,
so we are not sure whether the whole script is free of errors and completed successfully.

* Flexible output formats.
These include PDF, DOC/DOCX and HTML, with HTML having some notable benefits to other formats
including better usability for mobile devices like phones and tablets, and the ability to
support richer content such as interactive charts and dynamic tables (searchable, filterable, sortable).
Outputs include HTML documents, but also slideshows and even e-books.

These features make literate programming a quantum leap for science communication in a range of
situations.
Whether this is sharing a data analysis report, giving a presentation at a meeting,
writing a research article or self-publishing an e-book, literate programming provides the
ability to construct transparent data narratives with clear provenance in a convieniently sharable
form.

Literate programming also works neatly with the concept of the "executable paper", the idea that
the data analysis underlying an entire study can be reproduced with one or a few commands @Lasser2020-mm.
A typical genomics/bioinformatics study could involve one large literate script, or be broken down into
smaller scripts, where each one contains the code for generating a part of the article.
A multi-script approach would benefit from a master script that executes each component.
This further makes the job of reproducibility easier.

## Code version control and persistent sharing

A version control system (sometimes called "source control") is a type of program that "tracks changes"
made to sets of files, typically other computer program source code, scripts and documentation.
A set of files under version control is called a "repository" and represents a project or sub-project.
Version control is used extensively by software developers and is considered one of the key best
practices in software engineering.
Distributed version control systems involve a central web-accessible server to hosting a repository,
and each team member possesses a mirror copy on their local system.
Having a central, publicly available node assists in disseminating changes within teams and releasing
code to consumers.
There are many such distributed version control systems but `git` has emerged as the most popular
solution due to it's many powerful features, speed and large community and ecosystem.

Computational researchers can benefit significantly by adopting version control into their routine
practices.
Below I outline some of the benefits.

* Keeps a complete history of all code changes over time.
We always know which version of the script is the most up-to-date.
We can inspect and execute the code and reproduce the analysis at any particular point in the past.
Commit messages are retained, which gives a record of the motivations and purpose behind each
modification.

* Helps collaboration and project management.
Using a centralised code hosting platform such as "GitHub" or "BitBucket" can help manage contributions
from team members who may be working ascynchrously in different timezones and on different servers.
It also makes the user's work searchable, which is convenient when code snippets from a project
need to be reused for a new project.
These centralised platforms also enable discussions of code "issues" such as bugs and feature requests.
In bioinformatics teams, issues can be used to track the progress of a project through it's milestones
and allows team members to raise potential issues with a colleagues code (or one's own).

* Version control also works extremely well for documentation.
Books, educational resources and other data types are well served using version control (see below).

* Version control makes works easily sharable.
Work is easier to reproduce because the source folder contains a complete workflow with linked code
that doesn't need any modifications before being executed to reproduce some works.

* Distributed version control systems protect against code loss.
Computers sometimes fail and we sometimes delete or overwrite important files.
If the code is unrecoverable it can be a significant loss of work.
Although version control systems are not a back-up solution, they do add a layer of protection to the
code.
By preserving a copy of the repository on each team members computer in addition to the central
repo, means the code can be easily recovered if any one of those computers fail.

Need to mention long term (persistent) sharing.

## 2. Compute environment control

* Conda

* Containers: Docker/Guix/others

* Virtual machines

* Wasm

## 4. Documentation

* Instructions needed

* Requirements

* Credits

## 5. FAIR and persistent data sharing


* When designed correctly, the code "knows" where to access the associated metadata and data sets are,
which avoids "file not found" errors.
Metadata are usually small, so they can also be included in the repository as flat text files.
Small data files can also be included in the repository, although keep in mind that big data files
(>10MB) such as raw array scans, fastq files and the like are unsuitable for retention in a version
control repository as the large files will slow down code updates signifiantly.
Larger data files belong in a dedicated data repository, discussed below.
If the metadata and small data require some cleaning, then include the original and cleaned data in the
repository.
The code should refer only to the cleaned data and the original files are there only for reference.



* NCBI

* Zenodo

* Figshare

* Arweave/IPFS

* Portico

## Bringing it all together

Extreme reproducibility should be the new standard.

It generally doesn't matter what type of computer you're using.

The benefit is that even if one of the pillars breaks, the overall study could be reproduced
thanks to the remaining pillars.

More systematic studies to assess reproducible practices are required.



## Acknowledgements

I am grateful to the individuals who provided feedback on an [earlier draft of the five pillars
concept](https://twitter.com/mdziemann/status/1650371743113175040) including:

* Sriganesh Srihari

* Barbara Zdrazil

* Aziz Khan

* Pierre Poulain

* Irene López Santiago

## Bibliography

