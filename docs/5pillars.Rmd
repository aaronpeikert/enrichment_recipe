---
title: "The five pillars of computational reproducibility"
author: "Mark Ziemann"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    fig_width: 5
    fig_height: 5
bibliography: 5pillars.bib
csl: plos-computational-biology.csl
---

Mark Ziemann

Deakin University, Geelong, Australia, School of Life and Environmental Sciences.

Correspondance: m.ziemann@deakin.edu.au

## Summary

Computational reproducibility is in theory a simple premise, however in practice it is difficult to
achieve.
Building upon past efforts and proposals to maximise reproducibility and rigour, we
present a framework called the five pillars of reproducible computational research.
These are a combination of best practices which ensure that the computational research work can be
reproduced quickly and easily, and will endure relatively long time-frames.

## Introduction

```{r,libs,echo=FALSE}

library("DiagrammeR")

```

In research, computational reproducibility is the ability to use the materials from a past study
(such as data, code and documentation) to regenerate the outputs including figures and tables to
confirm the study's findings Goodman2016-wi.
Reproducibility is only the first step towards overall reliability, for example a study may be
reproducible, but suffer from analytical problems that invalidate the results.
If a study is reproducible, then at least these analytical issues can be more easily identified and
rectified.
So reproducibility is a necessary stepping-stone that in theory should enhance the overall reliability
of computational research including replicability and robustness.
Replicability being defined as the ability to repeat the entire study and come up with similar overall
results.
Robustness refers to the ability for the findings to be generalisable to other situations
@Nosek2022-zc.

Unreliable research is a waste of resources and time, and can mislead readers to pursue doomed
research directions.
Much has been written on how irreproducibility has reached crisis levels in psychology and life
sciences [@Baker2015-bs; @Begley2012-di].
Indeed in bioinformatics, the situation is grim with a 2009 report showing only a small fraction
of studies able to be reproduced @Ioannidis2009-hv, which brings into question the reliability of
those studies.
Irreproducible bioinformatics has the potential to place patient safety at risk, as demonstrated by an
earlier case study @Baggerly2010-nr.

As computational researchers, we are theoretically able to achieve 100% analytical reproducibility,
however in practice this is rarely achieved.
Data and code availability, ability to install software easily, and other factors mean that most
computational research works are unable to be reproduced.

A number of guides recommending enhanced computational reproducibility practices have emerged
[@Peng2011-ia; @Tan2010-is; @Sandve2013-lk; @Piccolo2016-ji; @Gruning2018-sg; @Lewis2016-qh;
@Brito2020-ld; @Rodrigues2023-fw; @Wilson2017-uq; @Akalin2018-eg ]
and this article attempts to unify these concepts and assemble a group of best practices where
computational reproducibility can be considered as complete, using a framework called the
five pillars of reproducible computational research (**Figure 1**).

The intended audience is bioinformatics data analysts (not tool developers), however the
principles described here should translate to other domains of study.

```{r,5pillars,fig.cap="Figure 1. The five pillars of computational reproducibility.",echo=FALSE,fig.height=6}

library("RColorBrewer")
cols <- brewer.pal(5, "Set2")
cols2 <- adjustcolor( cols, alpha.f = 0.7)
plot(0,type='n',axes=FALSE,ann=FALSE, xlim=c(0, 10), ylim=c(0, 8.5))
polygon(c(0,1.8,1.8,0),c(1,1,6,6), col=cols2[1], border = NA)
polygon(c(2,3.8,3.8,2),c(1,1,6,6), col=cols2[2], border = NA)
polygon(c(4,5.8,5.8,4),c(1,1,6,6), col=cols2[3], border = NA)
polygon(c(6,7.8,7.8,6),c(1,1,6,6), col=cols2[4], border = NA)
polygon(c(8,9.8,9.8,8),c(1,1,6,6), col=cols2[5], border = NA)
polygon(c(-0.2,10,10,-0.2),c(-0.4,-0.4,0.8,0.8),col="lightgray", border = NA)
polygon(c(-0.2,5,10),c(6.3,8.5,6.3),col="lightgray", border = NA)
points( c(0.9,2.9,4.9,6.9,8.9) , c(4.5,2.5,4.5,2.5,4.5), pch=19,cex=18,col=cols )
text(5,7,lab="Five pillars of reproducible computational research", cex=1.5)

labs <- c("Code version\ncontrol &\npersistent\n sharing",
  "Compute \nenvironment \ncontrol",
  "Literate\nprogramming",
  "Documentation",
  "FAIR &\npersistent\ndata sharing")

text( c(0.9,2.9,4.9,6.9,8.9) , c(4.5,2.5,4.5,2.5,4.5) , lab=labs)
text( 5 , 0.2 , lab="End-to-end code coverage")
text( 9, -0.2 , lab=expression(italic("created by Mark Ziemann")),cex=0.5)

```

## End-to-end code coverage

End-to-end code coverage is the foundation for reproducibility as the codebase should ideally contain all
instructions for completing a data analysis task.
This removes the need for any manual steps, which are not only time-consuming but also prone to errors.
Without end-to-end code coverage, most of the other best practices described here are not possible.
Scripted workflows, although not always free of errors, enable better auditing and easier reproduction
which would be difficult for graphical tools like spreadsheets.
Indeed spreadsheets are prone to data entry, manipulation and formula errors @Panko1996-ko, and are
somewhat overused in the life sciences.

Transfering data between compute platforms is also discouraged.
For example, having workflows that involve combinations of web-based and scripted tools require
data transfer steps which are inefficient and error-prone.

Related to this, the code and data need to be "linked", which means the code is aware of the location
of the data and how to get it.
The code should be able to fetch the data from a publicly accessible location and conduct the downstream
processing and analysis without having to aquire the data some other way.

A caveat for end-to-end code coverage is that sometimes manual data cleaning is unavoidable.
In those cases, retaining raw and cleaned data and providing a cleaning protocol is recommended,
then the computational workflow can begin with the cleaned dataset.

A guiding principle of the five pillars approach is that the publishing researchers should strive
to make the reproduction process easier and faster for those who wish to.

Getting started with R and python:

* Intro to R and Rstudio

* Intro to python and ipython IDE

* Intro to vscode

## Literate programming

Literate programming combines "chunks" of analytical code with human readable text @Knuth1984-di.
After compilation, the resulting output document contains the code together with computational results
such as figures and tables along with contextualising explanations and narratives.

The Sweave project, concieved in 2001, was designed to give LaTeX documents embedded R code chunks and
was envisaged as a way to generate R documentation materials and generate statistical analysis reports
@Leisch2002-yc.
Around 2015, R Markdown emerged as a more popular alternative, as formatting is simpler as compared to
Sweave, saving time.
Around the same time, Project Jupyter was developed to provide a "notebook" type interface, incorporating
R, Python, Julia and other computer language chunks into documents @Perkel2018-rz.
In 2022, the company behind RStudio (Posit) released Quarto, the conceptual successor of R Markdown,
but with enhanced support for other computer languages like Python, Julia, and Observable JavaScript
@Bauer2023-eh.

Whether Rmarkdown, Jupyter or Quarto, literate programming offers some substantial benefits over
alternative approaches:

* The provenance of any result can be demonstrated.
The resulting document produced contains the code executed, together with the results (eg: a chart).
This is in contrast to a data analysis report assembled in a word processor, which due to the many
copy-paste operations required might contain errors, or version mismatches.

* It saves time.
For the case where a report needs to be run routinely, this would involve significant copy-paste to
be assembled "manually" with a word processor.
Using a literate script would mean only minimal changes are required between interations.

* It accommodates extensive documentation.
This allows the analyst to include rich descriptions of scientific works.
For example, a report may contain background, methods, results, discussion and references.
Not only text, but various objects can be included like links to other resources,
tables, images, videos, etc.
More information on documentation below.

* Outputs are arranged.
When using a regular script, an analyst might make dozens of charts which are written to the
disk with names like "chart1.png" or "model8.svg".
When the number of outputs is large, it becomes unwieldy and hard to place which plot corresponds
to which part of the script.
By embedding the outputs (eg: charts and tables) in a document in the sequence that they were generated,
it helps the reader understand the logical steps taken in any analysis.

* Amenable to version control.
Version control is a useful best practice in software development (discussed below).

* Output reports are free from code errors.
The resulting document is only rendered when the entire script has compiled without errors.
In contrast, a regular script might generate some outputs and then encounter an error,
so we are not sure whether the whole script is free of errors and completed successfully.

* Flexible output formats.
These include PDF, DOC/DOCX and HTML, with HTML having some notable benefits to other formats
including better usability for mobile devices like phones and tablets, and the ability to
support richer content such as interactive charts and dynamic tables (searchable, filterable, sortable).
Outputs include HTML documents, but also slideshows and even e-books.

These features make literate programming a quantum leap for science communication in a range of
situations.
Whether this is sharing a data analysis report, giving a presentation at a meeting,
writing a research article or self-publishing an e-book, literate programming provides the
ability to construct transparent data narratives with clear provenance in a convieniently sharable
form.

Literate programming also works neatly with the concept of the "executable paper", the idea that
the data analysis underlying an entire study can be reproduced with one or a few commands @Lasser2020-mm.
A typical genomics/bioinformatics study could involve one large literate script, or be broken down into
smaller scripts, where each one contains the code for generating a part of the article.
A multi-script approach would benefit from a master script that executes each component.
This further makes the job of reproducibility easier.

Practical guides for jupyter and R Markdown - put these on webarchive and provide persistent links.

* https://medium.com/@itsjzt/beginner-guide-to-markdown-229adce30074

* https://www.dataquest.io/blog/r-markdown-guide-cheatsheet/

* https://tysonbarrett.com/Rstats/chapter-9-reproducible-workflow-with-rmarkdown.html

* https://andrewbtran.github.io/NICAR/2017/reproducibility-rmarkdown/rmarkdown.html

* https://quarto.org/docs/computations/r.html

* https://quarto.org/docs/computations/python.html

* https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/

* https://www.dataquest.io/blog/jupyter-notebook-tutorial/

* https://medium.com/velotio-perspectives/the-ultimate-beginners-guide-to-jupyter-notebooks-6b00846ed2af

## Code version control and persistent sharing

A version control system (sometimes called "source control") is a type of program that "tracks changes"
made to sets of files, typically other computer program source code, scripts and documentation.
A set of files under version control is called a "repository" and represents a project or sub-project.
Version control is used extensively by software developers and is considered one of the key best
practices in software engineering.
Distributed version control systems (DVCS) involve a central web-accessible server to hosting a
repository, and each team member possesses a mirror copy on their local system.
Having a central, publicly available node assists in disseminating changes within teams and releasing
code to consumers.
There are many such DVCSs but `git` has emerged as the most popular
solution due to it's many powerful features, speed/efficiency and large community and ecosystem.

Computational researchers can benefit significantly by adopting DVCS into their routine
practices.
Below I outline some of the benefits.

* Keeps a complete history of all code changes over time.
We always know which version of the script is the most up-to-date.
We can inspect and execute the code and reproduce the analysis at any particular point in the past.
Commit messages are retained, which gives a record of the motivations, purpose and person behind each
modification.

* Helps collaboration and project management.
Using a centralised code hosting platform such as "GitHub" or "BitBucket" can help manage contributions
from team members who may be working ascynchrously in different timezones and on different servers.
It also makes the user's work searchable, which is convenient when code snippets from a project
need to be reused for a new project.
These centralised platforms also enable discussions of code "issues" such as bugs and feature requests.
In bioinformatics teams, issues can be used to track the progress of a project through it's milestones
and allows team members to raise potential issues with a colleagues code (or one's own).

* Supports documentation best practices.
Books, educational resources and other data types are well served using DVCS (see below).

* Makes works easily sharable.
Work is easier to reproduce because the source folder contains a complete workflow with linked code
that doesn't need any modifications before being executed to reproduce some works.

* Protect against code loss.
Computers sometimes fail and we sometimes delete or overwrite important files.
If the code is unrecoverable it can be a significant loss of work that is costly to replace.
Although DVCSs are not a back-up solution, they do add a layer of protection to the
code.
By preserving a copy of the repository on each team members computer in addition to the central
repo, means the code can be easily recovered if any one of those computers fail.

Although DVCSs assist with code sharing, they are not considered a long term solution and might not
survive the "ten year challenge" of code reproducibility @Perkel2020-io.
To achieve that, the code needs to be deposited to a long term
repository.
Zenodo and Figshare repositories are both good options for long term archiving of software, and the
link to these should be provided in the respective journal article/pre-print.

Practical guides for `git` and GitHub - put these on webarchive and provide persistent links.

* https://www.datacamp.com/tutorial/github-and-git-tutorial-for-beginners/

* https://www.freecodecamp.org/news/introduction-to-git-and-github/

* https://www.geo.uzh.ch/microsite/reproducible_research/post/rr-rstudio-git/

* https://anderfernandez.com/en/blog/how-to-link-and-use-git-with-rstudio/

* https://andgarc.github.io/basic-git-jupyter-lesson/index.html

* https://blog.reviewnb.com/github-jupyter-notebook/

* https://andgarc.github.io/basic-git-jupyter-lesson/index.html

## Compute environment control

Most popular software undergoes regular updates to patch bugs and add new features.
Bioinformatics software is no different, and it is well known that such changes have the
potential to cause changes to results @Seyednasrollah2015-cc.
This is why it is best practice to report the exact version of all programs used in an analysis
(and packages therein) and even make archival copies for future reference @Sandve2013-lk.

In R, such reporting is possible using the `sessionInfo()` command;
while for python this is possible using the `session_info` package.
Using literate programming ensures that the output document contains a record of this important
information.

Although regular software updates are overall a good thing, it poses a problem for future
reproducibility.
A researcher trying to reproduce a 10 year old study in R v3.0 could have a headache, as they would need
to roll back their R version, and possibly their operating system as well, as R and other languages
requires certain system libraries like BLAS and LAPACK for low-level routines.

To avoid this, a virtual machine (VM) could be used to run a system-in-a-system.
This means that the "host" machine can run another "guest" operating system with the right R version,
without needing to change the host R version.
While this provides good reproducibility, setting up an environment with a 10 year old operating system,
R and packages would take a few hours to accomplish.
Researchers could take a snapshot of their VM system and share it to help reproducibility and
auditability, however the size of these images is relatively large due to the fact it contains the OS,
software stack and project data.
Moreover, the performance of computation in guest system is typically slower than when run direct on
the host.

Another potential solution to this is to use a package environment management system
(eg: Conda or Guix).
These allow users to easily create, manage, and deploy software packages,
dependencies, and environments across different computing platforms @Perkel2023-cv.
Conda was initially developed to solve the problem of package dependency management in Python,
but it now supports many other programming languages, including R, C/C++, Java, and more.
Conda allows researchers to create isolated environments with specific versions of
packages, so users can have different versions of R or python in different environments
on the same host system.
Conda however does not typically disseminate the research code, as it is up to the reproducer to
set that up inside the environment.
Moreover Conda environments have limitations around portability and consistency, as Conda environments
are tied to specific operating systems and hardware architecture.
Guix on the other hand, gives system-level reproducibility, but has a smaller community and ecosystem
of supported packages.

Containers are an attempt to resolve some of the downsides of virtual machines and package managers.
Container images can be thought of as similar to virtual machines, but are more lightweight as they
share parts of the host operating system.
This means that execution in a container entails only a small overhead as compared to virtual machines
where the overhead is much larger, meaning that performance in containers is similar to the host system.
Containers provide a consistent runtime environment, ensuring that applications runs the same way
across different types of systems and environments.
This is because containers include everything needed to run the application, including the system tools
and libraries, ensuring that the environment is consistent across different systems.
For example, the most popular containerisation system, Docker, has cross platform support, so it is
possible to run Windows and Linux/Unix containers on any computer with Docker installed, with the
promise of reproducibility.
While there are several alternatives such as Podman and Rkt, Docker remains the most widely used
containerisation system for web development and research.
Due to its early adoption, Docker now has a large community of users, extensive documentation and
a vast collection of pre-built container images called DockerHub.
Docker can fetch images from DockerHub and run them on the host system with just a couple of commands,
and typically within a few minutes.
This accellerates the installation procedure dramatically which is a known bottleneck for bioinformatic
reproducibility.

We are also watching with interest the early development of WebAssembly (Wasm) based approaches for
compute environment control.
The ability to reproduce operating systems, programming languages and workflows in the browser
opens up intriguing possibilities for more widespread reproduction and auditing without the need to
install any software at all.
For example, it is now possible embed a Wasm-enabled R terminal inside a Quarto HTML document, which
enables in-browser reproducibility of R workflows @Balamuta2023-ma.

## Documentation

* Instructions needed

* Requirements

* Credits

## FAIR and persistent data sharing


* When designed correctly, the code "knows" where to access the associated metadata and data sets are,
which avoids "file not found" errors.
Metadata are usually small, so they can also be included in the repository as flat text files.
Small data files can also be included in the repository, although keep in mind that big data files
(>10MB) such as raw array scans, fastq files and the like are unsuitable for retention in a version
control repository as the large files will slow down code updates signifiantly.
Larger data files belong in a dedicated data repository, discussed below.
If the metadata and small data require some cleaning, then include the original and cleaned data in the
repository.
The code should refer only to the cleaned data and the original files are there only for reference.

Archive versions of critical reference datasets.

* NCBI

* Zenodo

* Figshare

* Arweave/IPFS

* Portico

## Bringing it all together

Computational reproducibility of
Jupyter notebooks from biomedical
publications
https://arxiv.org/pdf/2209.04308.pdf

RESEARCHOPS: A. PRINCIPLED FRAMEWORK. AND GUIDE TO. COMPUTATIONAL. REPRODUCIBILITY.
https://osf.io/ubj76/download

AlNoamany, Yasmin, and John A. Borghi. 2018. “Towards Computational Reproducibility: Researcher Perspectives on the Use and Sharing of
Software.” Article. PEERJ COMPUTER SCIENCE, September. https://doi.org/ghkb9j (https://doi.org/ghkb9j).

Archmiller, Althea A., Andrew D. Johnson, Jane Nolan, Margaret Edwards, Lisa H. Elliott, Jake M. Ferguson, Fabiola Iannarilli, et al. 2020.
“Computational Reproducibility in the Wildlife Society’s Flagship Journals.” Article. JOURNAL OF WILDLIFE MANAGEMENT 84 (5): 1012–17.
https://doi.org/gg66q7 (https://doi.org/gg66q7).

Beaulieu-Jones, Brett K, and Casey S Greene. 2017. “Reproducibility of Computational Workflows Is Automated Using Continuous Analysis.”
Nature Biotechnology, no. 4: 342.

Catlin, Ann Christine, Chandima HewaNadungodage, and Andres Bejarano. 2019. “Lifecycle Support for Scientific Investigations: Integrating
Data, Computing, and Workflows.” Article. COMPUTING IN SCIENCE & ENGINEERING 21 (4): 49–61. https://doi.org/gjt725 (https://doi.org
/gjt725).

Culina, Antica et al. 2018. “Navigating the Unfolding Open Data Landscape in Ecology and Evolution.” Nature Ecology & Evolution, 1–7.
de Bayser, Maximilien, Leonardo G. Azevedo, and Renato Cerqueira. 2015. “ResearchOps: The Case for DevOps in Scientific Applications.” In
2015 IFIP/IEEE International Symposium on Integrated Network Management (IM), 1398–1404. Ottawa, ON, Canada: IEEE. https://doi.org
/ghkb9s (https://doi.org/ghkb9s).

Díaz, Jessica, Jorge Pérez-Martínez, Agustin Yague, Andrea Villegas, and Antonio Antona. 2019. DevOps in Practice – A Preliminary Analysis of
Two Multinational Companies.

Hatton, L., and M. Van Genuchten. 2019. “Computational Reproducibility: The Elephant in the Room.” IEEE Software 36 (2): 137–44.
https://doi.org/ggkvtr (https://doi.org/ggkvtr).

Hocquet, Alexandre, and Frederic Wieber. 2021a. “Epistemic Issues in Computational Reproducibility: Software as the Elephant in the Room.”
Article. EUROPEAN JOURNAL FOR PHILOSOPHY OF SCIENCE 11 (2). https://doi.org/gkm94m (https://doi.org/gkm94m).

Hocquet, Alexandre, and Frédéric Wieber. 2021b. “Epistemic Issues in Computational Reproducibility: Software as the Elephant in the Room.”
European Journal for Philosophy of Science 11 (2). https://doi.org/gkm94m (https://doi.org/gkm94m).

Koehler Leman, Julia, Brian D. Weitzner, P. Douglas Renfrew, Steven M. Lewis, Rocco Moretti, Andrew M. Watkins, Vikram Khipple Mulligan, et al.
2020. “Better Together: Elements of Successful Scientific Software Development in a Distributed Collaborative Community.” PLoS
Computational Biology 16 (5): e1007507. https://doi.org/ggt62r (https://doi.org/ggt62r).

Leipzig, Jeremy, Daniel Nüst, Charles Tapley Hoyt, Stian Soiland-Reyes, Karthik Ram, and Jane Greenberg. 2020. “The Role of Metadata in
Reproducible Computational Research.”

Maer-Matei, Monica Mihaela, Tiberiu Marian Georgescu, Cristina Mocanu, and Ana-Maria Zamfir. 2019/01/01////. “Skill Needs for Early Career
Researchers.” Sustainability 11 (10). https://doi.org/ghkb9m (https://doi.org/ghkb9m).

Murray, NJ et al. 2021. “Data Freshness in Ecology and Conservation.” Trends Ecol Evol 36 (6): 485–87.

Obels, Pepijn, Daniël Lakens, Nicholas Coles, Jaroslav Gottfried, and Seth Green. 2020. “Analysis of Open Data and Computational

Reproducibility in Registered Reports in Psychology.” Advances in Methods and Practices in Psychological Science 3 (May):
251524592091887. https://doi.org/gg4vw4 (https://doi.org/gg4vw4).

Peikert, Aaron, and Andreas Markus Brandmaier. 2019. “A Reproducible Data Analysis Workflow with R Markdown, Git, Make, and Docker.”

PsyArXiv. https://doi.org/10.31234/osf.io/8xzqy (https://doi.org/10.31234/osf.io/8xzqy).
@aaron_willcox ! " # || @Elliot_Gould_ ! " #




We provide some practical resources for those interested in adopting various

| Topic | Resource |
| --- | --- |
| General coding for science | eg |
| Version control | eg |
| Literate programming | eg |
| Environment control | eg |
| Documentation | eg |
| Data sharing | eg |

Extreme reproducibility should be the new standard.

It generally doesn't matter what type of computer you're using.

The benefit is that even if one of the pillars breaks, the overall study could be reproduced
thanks to the remaining pillars.

More systematic studies to assess reproducible practices are required.

More step-by-step guides required for frequently used routines.

Adoption of these practices would require a change in incentives. https://www.nature.com/articles/s41562-016-0021

## Acknowledgements

I am grateful to the individuals who provided feedback on an [earlier draft of the five pillars
concept](https://twitter.com/mdziemann/status/1650371743113175040) including:

* Sriganesh Srihari @srigsri23

* Barbara Zdrazil @BZdrazil

* Aziz Khan @khanaziz84

* Pierre Poulain @pierrepo

* Irene López Santiago @IreneLopSci

## Bibliography

