https://davetang.org/muse/2019/12/04/reproducible-bioinformatics/

The BioContainers initiative https://biocontainers.pro/

Building containerized workflows using the BioDepot-workflow-builder (Bwb)
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6883158/

Packaging, containerization, and virtualization of computational omics methods:
Advances, challenges, and opportunities
https://arxiv.org/pdf/2203.16261.pdf

Practical Computational Reproducibility in the Life Sciences
https://www.sciencedirect.com/science/article/pii/S2405471218301406

A path to moderate the reproducibility crisis in bioinformatics
http://www.bioinformatics.deib.polimi.it/geco/workshop/reproducibility.pdf
www.reproducible-bioinformatics.org
https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2296-x

Ten Simple Rules for Reproducible Computational Research
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285

The reproducibility crisis in science: A statistical counterattack (Roger Peng)
https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2015.00827.x

Reproducible research in computational science
https://pubmed.ncbi.nlm.nih.gov/22144613/

Reproducibility standards for machine learning in the life sciences
https://www.nature.com/articles/s41592-021-01256-7#Sec4

Tools and techniques for computational reproducibility 
https://academic.oup.com/gigascience/article/5/1/s13742-016-0135-4/2720991?login=false

Reproducible, scalable, and shareable analysis pipelines with bioinformatics workflow managers
https://www.nature.com/articles/s41592-021-01254-9

Compi: a framework for portable and reproducible pipelines
https://pubmed.ncbi.nlm.nih.gov/34239974/

Publishing computational research - a review of infrastructures for reproducible and transparent scholarly communication
https://pubmed.ncbi.nlm.nih.gov/32685199/

Container-based bioinformatics with Pachyderm
https://pubmed.ncbi.nlm.nih.gov/30101309/

Simplifying the development of portable, scalable, and reproducible workflows
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8514239/

Easyreporting simplifies the implementation of Reproducible Research layers in R software
https://pubmed.ncbi.nlm.nih.gov/33970927/


# deleted text from example.Rmd

Previously my team have written about methodological problems in enrichment analysis.
These include statistical problems, like lack of FDR correction, lack of correct background
 and poor reporting of methodological details (Wijesooriya et al, 2022).

Therefore there is a need for community-agreed guidelines for conducting and presenting
enrichment analysis work, which is something that has been noted by others
(Timmons et al, 2015; Reimand et al 2019), but has only recently been addressed (Zhao & Rhee 2023).

The purpose of this work is to offer a postive examplar of an enrichment analysis
that fills all criteria:

1. Statistically rigourous.

2. Well documented in the manuscript and public code.

3. Maximally reproducible.

While rigour and good documentation are features of enrichment analyses in high quality specialist
genomics journals, reproducibility remains a distinct weakness.

When trying to reproduce or replicate the work of others we are often frustrated by
the unavailability of input data, of functional annotations, of code, and full descriptions
of results.

For example, while conducting a replication study of 20 journal articles, we found that
one of the most frequently used web-tools, DAVID bioinformatics, was going to move from
version 6.7 to version 6.8, with no option to continue using v6.7.

Changes such as these mean that the results of many journal articles can no longer be
independently replicated, only a couple of years after original publication.

As time passes and software versions update, it will become ever more difficult to replicate
previous analyses.
This means that it will be impossible to audit many research articles if those findings come
under question.

Although it doesn't need to be this way due to the precise nature of modern computers and software.
In order for bioinformatics analysis to be 100% reproducible requires a few key elements.
These are:

* Input data

* Set of instructions (analysis code).

* Operating system

With these things working together, it is possible for a data analysis workflow to be 100%
replicable and auditable for many years into the future.

To achieve this requires some planning:

1. Input data needs to be deposited to a public and persistent location.

2. Analysis code packaged in a working Docker container.

A Docker container is a minimal operating system that has the necessary software to complete a task.
Docker is used extensively in web development, to rapidly set up many modular services into
full stacks of functionality.
For example using Docker containers, a website front end and back end can be set up once, and then
deployed at huge scale (eg thousands of instances) with minimal work involved.

In addition, Docker containers ensure consistent behaviour across different types of computing
infrastructure, so it doesn't matter whether the container is run on a linux workstation,
desktop windows PC or cloud server, the end results should be identical.

While there have been many general guides to computatonal reproducibility in the literature,
there is a need for boilerplate code templates that researchers can quickly adapt/tweak for their own
projects.

The demand for this is expected to be huge, as there are massive numbers of published articles featuring
such analyses.
Just in 2022, there were 9,917 PubMed articles with the keywords pathway/enrichment/ontology analysis
in the title or abstract alone.

The more rigourous, well documented and reproducible these studies are, we will find less wasted
research resources and greater trust/support from the general community.

